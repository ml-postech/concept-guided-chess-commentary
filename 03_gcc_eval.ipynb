{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from os.path import *\n",
    "import importlib\n",
    "\n",
    "import yaml\n",
    "import jsonlines\n",
    "import pickle\n",
    "import wandb\n",
    "import logging\n",
    "import tqdm\n",
    "\n",
    "import chess\n",
    "import re\n",
    "\n",
    "%matplotlib inline    \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "sys.path.append(f\"{os.getcwd()}/lczeroTraining/tf/\")\n",
    "\n",
    "from stockfish import Stockfish\n",
    "from tfprocess import TFProcess\n",
    "from lcztools import LeelaBoard as leelaBoard\n",
    "\n",
    "probing_svm = __import__(\"01_probing_svm\", fromlist=\"*\")\n",
    "\n",
    "%matplotlib inline    \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast \n",
    "\n",
    "log_file = \"results/pipeline_v9.log\"\n",
    "target_fens = []\n",
    "target_moves = []\n",
    "base_responses = []\n",
    "engine_responses = []\n",
    "concept_responses = []\n",
    "\n",
    "with open(log_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        data_tuple = ast.literal_eval(line.strip())\n",
    "\n",
    "        target_position, target_move_full, vanilla_response, engine_response, response1, response2 = data_tuple\n",
    "\n",
    "        target_fens.append(target_position)\n",
    "        target_moves.append(target_move_full)\n",
    "        base_responses.append((None, vanilla_response.replace(\"vanilla \", \"\")))  \n",
    "        engine_responses.append((None, engine_response.replace(\"engine \", \"\")))  \n",
    "        concept_responses.append((None, response1, response2))  \n",
    "\n",
    "target_moves_strip = list(map(lambda x: x.split()[1], target_moves))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "tmp = target_fens[idx].split(\"- -\")\n",
    "target_position = tmp[0] + \"KQkq -\" + tmp[1]\n",
    "svg = plot_board(target_position, target_moves_strip[idx], is_san=True)\n",
    "\n",
    "# svg = plot_board(target_fens[idx], target_moves_strip[idx], is_san=True)\n",
    "print(ref_list[idx].replace(\"\\n\", \" \"))\n",
    "# print(gac_list[idx].replace(\"\\n\", \" \"))\n",
    "print(base_responses[idx][1].replace(\"comment: \", \"\").replace(\"\\n\", \" \"))\n",
    "print(engine_responses[idx][1].replace(\"comment: \", \"\").replace(\"\\n\", \" \"))\n",
    "print(concept_responses[idx][1])\n",
    "print(concept_responses[idx][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"#######################\"\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise TypeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_list = [6, 11, 16, 22, 51, 54, 59, 60, 66, 67, 74, 77, 78, 80, 83, 84, 95, 97, 100, 123, 127, 130, 131, 136, 137, 138, 139, 165, 169, 170, 173, 175, 176, 177, 181, 186, 188, 189, 200, 202, 206, 210, 212, 219, 225, 226, 227, 231, 232, 238]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevance \n",
    "\n",
    "relevance_scores = {\n",
    "    \"ref\": [],\n",
    "    # \"gac\": [],\n",
    "    \"gpt\": [],\n",
    "    \"gpt_engine\": [],\n",
    "    \"gpt_concept\": [],\n",
    "    \"gpt_concept_wrong\": [],\n",
    "}\n",
    "\n",
    "for idx in range(len(target_fens)):\n",
    "    for target, target_list in [\n",
    "            (\"ref\", ref_list), \n",
    "            # (\"gac\", gac_list), \n",
    "            (\"gpt\", [r[1].replace(\"comment: \", \"\") for r in base_responses]), \n",
    "            (\"gpt_engine\", [r[1].replace(\"comment: \", \"\") for r in engine_responses]), \n",
    "            (\"gpt_concept\", [r[2].split(\"comment: \")[1] for r in concept_responses]),\n",
    "        ]:\n",
    "        text_to_eval = target_list[idx].replace(\"\\n\", \" \")\n",
    "        text_ref = ref_list[idx].replace(\"\\n\", \" \")\n",
    "        print(text_ref)\n",
    "        print(text_to_eval)\n",
    "\n",
    "        target_position = target_fens[idx]\n",
    "        target_move = target_moves_strip[idx]\n",
    "        target_move_full = target_moves[idx]\n",
    "        if \"O-O\" in target_move:\n",
    "            tmp = target_position.split(\"- -\")\n",
    "            target_position = tmp[0] + \"KQkq -\" + tmp[1]\n",
    "        attacks = get_all_attacks(target_position, after_move=target_move)\n",
    "        engine_eval = get_engine_evalutaion(target_position, target_move).replace(\"evaluation: \", \"\")\n",
    "\n",
    "        prompt = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You will be given two comments about a chess move.\n",
    "\n",
    "        Your task is to rate the comment on one metric.\n",
    "\n",
    "        Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "        Evaluation Criteria:\n",
    "\n",
    "        Relevance (1-5) - Relevence of the target comment to important aspects of the chess move. The comment should include only information relevant to the chess move or reasoning for taking or not taking the chess move. An engine evaluation result and a reference comment is given as a hint. \n",
    "\n",
    "        Evaluation Steps:\n",
    "\n",
    "        1. Read the commment and the reference comment carefully.\n",
    "        2. Read the chess position and move carefully, and find out important aspects based on the reference.\n",
    "        2. Assess if every expressen of the comment is relevant to the important information about the chess move.\n",
    "        3. Assign a Relevance score from 1 to 5.\n",
    "\n",
    "        \"\"\"},\n",
    "        # Relevance (1-5) - Relevence of a target comment to a reference comment. The comment should include only information relevant to the chess move or reasoning for taking or not taking the chess move. An engine evaluation result is given as a hint.\n",
    "        {\"role\": \"user\", \"content\": (\n",
    "            f\"position:\\n{target_position}\\n\\n\"\n",
    "            f\"move:\\n{target_move_full}\\n\\n\"\n",
    "            f\"target comment:\\n\\n{text_to_eval}\\n\\n\"\n",
    "            f\"reference comment:\\n\\n{text_ref}\\n\\n\"\n",
    "            f\"engine evaluation:\\n\\n{engine_eval}\\n\\n\"\n",
    "            \"Score(1-5, score ONLY): \"\n",
    "        )},\n",
    "        ]\n",
    "        print(prompt)\n",
    "        response = client.chat.completions.create(model=\"gpt-4o\", messages=prompt, logprobs=True, top_logprobs=10)\n",
    "        response_engine_attack = response.choices[0].message.content\n",
    "    \n",
    "        token_probs = [(response.choices[0].logprobs.content[0].top_logprobs[i].token, np.exp(response.choices[0].logprobs.content[0].top_logprobs[i].logprob)) for i in range(10)]\n",
    "        norm_factor = sum([p for t, p in token_probs if t in ['1', '2', '3', '4', '5']])\n",
    "        score = sum([int(t) * p / norm_factor for t, p in token_probs if t in ['1', '2', '3', '4', '5']])\n",
    "\n",
    "        print(score)\n",
    "        relevance_scores[target].append(score)\n",
    "\n",
    "print(relevance_scores)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completeness\n",
    "\n",
    "completeness_scores = {\n",
    "    \"ref\": [],\n",
    "    # \"gac\": [],\n",
    "    \"gpt\": [],\n",
    "    \"gpt_engine\": [],\n",
    "    \"gpt_concept\": [],\n",
    "    \"gpt_concept_wrong\": [],\n",
    "}\n",
    "\n",
    "# for idx in [1, 3, 4,]:\n",
    "for idx in idx_list:\n",
    "# for idx in range(len(target_fens)):\n",
    "    for target, target_list in [\n",
    "            (\"ref\", ref_list), \n",
    "            # (\"gac\", gac_list), \n",
    "            (\"gpt\", [r[1].replace(\"comment: \", \"\") for r in base_responses]), \n",
    "            (\"gpt_engine\", [r[1].replace(\"comment: \", \"\") for r in engine_responses]), \n",
    "            (\"gpt_concept\", [r[2].split(\"comment: \")[1] for r in concept_responses]),\n",
    "        ]:\n",
    "        text_to_eval = target_list[idx].replace(\"\\n\", \" \")\n",
    "        text_ref = ref_list[idx].replace(\"\\n\", \" \")\n",
    "        print(text_to_eval)\n",
    "\n",
    "        target_position = target_fens[idx]\n",
    "        target_move = target_moves_strip[idx]\n",
    "        target_move_full = target_moves[idx]\n",
    "        if \"O-O\" in target_move:\n",
    "            tmp = target_position.split(\"- -\")\n",
    "            target_position = tmp[0] + \"KQkq -\" + tmp[1]\n",
    "        attacks = get_all_attacks(target_position, after_move=target_move)\n",
    "        engine_eval = get_engine_evalutaion(target_position, target_move)\n",
    "\n",
    "        prompt = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You will be given single comment about a chess move.\n",
    "\n",
    "        Your task is to rate the comment on one metric.\n",
    "\n",
    "        Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "        Evaluation Criteria:\n",
    "\n",
    "        Completeness (1-5) - Completeness of a comment. The comment should cover all critical points on the chess board, ensuring that no important factors are overlooked. An engine evaluation result is given as a hint.\n",
    "\n",
    "        Evaluation Steps:\n",
    "\n",
    "        1. Read the two commments carefully.\n",
    "        2. Assess how well the comment addresses the important information, and how well the comment covers the entire important information without missing any.\n",
    "        3. Assign a Completeness score from 1 to 5.\n",
    "\n",
    "        \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": (\n",
    "            f\"position:\\n{target_position}\\n\\n\"\n",
    "            f\"move:\\n{target_move_full}\\n\\n\"\n",
    "            f\"comment:\\n\\n{text_to_eval}\\n\\n\"\n",
    "            f\"engine evaluation:\\n\\n{engine_eval}\\n\\n\"\n",
    "            \"Score(1-5, score ONLY): \"\n",
    "        )},\n",
    "        ]\n",
    "        print(prompt)\n",
    "        response = client.chat.completions.create(model=\"gpt-4o\", messages=prompt, logprobs=True, top_logprobs=10)\n",
    "        response_engine_attack = response.choices[0].message.content\n",
    "    \n",
    "        token_probs = [(response.choices[0].logprobs.content[0].top_logprobs[i].token, np.exp(response.choices[0].logprobs.content[0].top_logprobs[i].logprob)) for i in range(10)]\n",
    "        norm_factor = sum([p for t, p in token_probs if t in ['1', '2', '3', '4', '5']])\n",
    "        score = sum([int(t) * p / norm_factor for t, p in token_probs if t in ['1', '2', '3', '4', '5']])\n",
    "\n",
    "        print(score)\n",
    "        completeness_scores[target].append(score)\n",
    "\n",
    "print(completeness_scores)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clarity\n",
    "\n",
    "clarity_scores = {\n",
    "    \"ref\": [],\n",
    "    # \"gac\": [],\n",
    "    \"gpt\": [],\n",
    "    \"gpt_engine\": [],\n",
    "    \"gpt_concept\": [],\n",
    "}\n",
    "\n",
    "# for idx in [1, 3, 4,]:\n",
    "# for idx in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]:\n",
    "for idx in idx_list:\n",
    "# for idx in range(len(target_fens)):\n",
    "    for target, target_list in [\n",
    "            (\"ref\", ref_list), \n",
    "            # (\"gac\", gac_list), \n",
    "            (\"gpt\", [r[1].replace(\"comment: \", \"\") for r in base_responses]), \n",
    "            (\"gpt_engine\", [r[1].replace(\"comment: \", \"\") for r in engine_responses]), \n",
    "            (\"gpt_concept\", [r[2].split(\"comment: \")[1] for r in concept_responses]),\n",
    "        ]:\n",
    "        text_to_eval = target_list[idx].replace(\"\\n\", \" \")\n",
    "        text_ref = ref_list[idx].replace(\"\\n\", \" \")\n",
    "        print(text_to_eval)\n",
    "\n",
    "        target_position = target_fens[idx]\n",
    "        target_move = target_moves_strip[idx]\n",
    "        target_move_full = target_moves[idx]\n",
    "        if \"O-O\" in target_move:\n",
    "            tmp = target_position.split(\"- -\")\n",
    "            target_position = tmp[0] + \"KQkq -\" + tmp[1]\n",
    "        attacks = get_all_attacks(target_position, after_move=target_move)\n",
    "        engine_eval = get_engine_evalutaion(target_position, target_move).replace(\"evaluation: \", \"\")\n",
    "\n",
    "        prompt = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You will be given single comment about a chess move.\n",
    "\n",
    "        Your task is to rate the comment on one metric.\n",
    "\n",
    "        Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "        Evaluation Criteria:\n",
    "\n",
    "        Clarity (1-5) - Clarity of a comment. The comment should be clear and detailed, without vague or ambiguous statements.\n",
    "\n",
    "        Evaluation Steps:\n",
    "\n",
    "        1. Read the commment carefully.\n",
    "        2. Assess how the comment is clear and detailed, without vague or ambiguous statements.\n",
    "        3. Assign a Clarity score from 1 to 5.\n",
    "\n",
    "        \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": (\n",
    "            f\"position:\\n{target_position}\\n\\n\"\n",
    "            f\"move:\\n{target_move_full}\\n\\n\"\n",
    "            f\"comment:\\n\\n{text_to_eval}\\n\\n\"\n",
    "            \"Score(1-5, score ONLY): \"\n",
    "        )},\n",
    "        ]\n",
    "        print(prompt)\n",
    "        response = client.chat.completions.create(model=\"gpt-4o\", messages=prompt, logprobs=True, top_logprobs=10)\n",
    "        response_engine_attack = response.choices[0].message.content\n",
    "    \n",
    "        token_probs = [(response.choices[0].logprobs.content[0].top_logprobs[i].token, np.exp(response.choices[0].logprobs.content[0].top_logprobs[i].logprob)) for i in range(10)]\n",
    "        norm_factor = sum([p for t, p in token_probs if t in ['1', '2', '3', '4', '5']])\n",
    "        score = sum([int(t) * p / norm_factor for t, p in token_probs if t in ['1', '2', '3', '4', '5']])\n",
    "\n",
    "        print(score)\n",
    "        clarity_scores[target].append(score)\n",
    "\n",
    "print(clarity_scores)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fluency \n",
    "\n",
    "fluency_scores = {\n",
    "    \"ref\": [],\n",
    "    # \"gac\": [],\n",
    "    \"gpt\": [],\n",
    "    \"gpt_engine\": [],\n",
    "    \"gpt_concept\": [],\n",
    "}\n",
    "\n",
    "# for idx in idx_list:\n",
    "for idx in range(len(target_fens)):\n",
    "    for target, target_list in [\n",
    "            (\"ref\", ref_list), \n",
    "            # (\"gac\", gac_list), \n",
    "            (\"gpt\", [r[1].replace(\"comment: \", \"\") for r in base_responses]), \n",
    "            (\"gpt_engine\", [r[1].replace(\"comment: \", \"\") for r in engine_responses]), \n",
    "            (\"gpt_concept\", [r[2].split(\"comment: \")[1] for r in concept_responses]),\n",
    "        ]:\n",
    "        text_to_eval = target_list[idx].replace(\"\\n\", \" \")\n",
    "        text_ref = ref_list[idx].replace(\"\\n\", \" \")\n",
    "        print(text_to_eval)\n",
    "\n",
    "        target_position = target_fens[idx]\n",
    "        target_move = target_moves_strip[idx]\n",
    "        target_move_full = target_moves[idx]\n",
    "        if \"O-O\" in target_move:\n",
    "            tmp = target_position.split(\"- -\")\n",
    "            target_position = tmp[0] + \"KQkq -\" + tmp[1]\n",
    "        attacks = get_all_attacks(target_position, after_move=target_move)\n",
    "        engine_eval = get_engine_evalutaion(target_position, target_move)\n",
    "\n",
    "        prompt = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You will be given one comment written for a chess move.\n",
    "\n",
    "        Your task is to rate the comment on one metric.\n",
    "\n",
    "        Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "\n",
    "        Evaluation Criteria:\n",
    "\n",
    "        Fluency (1-5): Fluency of a comment.\n",
    "\n",
    "        1. Read the commment carefully.\n",
    "        2. Assess the sentences of comment is coherently organized. The comment should contain well-structured language and coherent transitions.\n",
    "        3. Assign a Fluency score from 1 (not readable) to 5 (very fluent).\n",
    "        \n",
    "        \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": (\n",
    "            f\"target comment:\\n\\n{text_to_eval}\\n\\n\"\n",
    "            \"Score(1-5, score ONLY): \"\n",
    "        )},\n",
    "        ]\n",
    "        print(prompt)\n",
    "        response = client.chat.completions.create(model=\"gpt-4o\", messages=prompt, logprobs=True, top_logprobs=10)\n",
    "        response_engine_attack = response.choices[0].message.content\n",
    "    \n",
    "        token_probs = [(response.choices[0].logprobs.content[0].top_logprobs[i].token, np.exp(response.choices[0].logprobs.content[0].top_logprobs[i].logprob)) for i in range(10)]\n",
    "        norm_factor = sum([p for t, p in token_probs if t in ['1', '2', '3', '4', '5']])\n",
    "        score = sum([int(t) * p / norm_factor for t, p in token_probs if t in ['1', '2', '3', '4', '5']])\n",
    "\n",
    "        print(score)\n",
    "        fluency_scores[target].append(score)\n",
    "\n",
    "print(fluency_scores)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print({src: sum(val) / len(val) for src, val in relevance_scores.items() if len(val) != 0})\n",
    "print({src: sum(val) / len(val) for src, val in completeness_scores.items() if len(val) != 0})\n",
    "print({src: sum(val) / len(val) for src, val in clarity_scores.items() if len(val) != 0})\n",
    "print({src: sum(val) / len(val) for src, val in fluency_scores.items() if len(val) != 0})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
